{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hjjung113/anaconda3/envs/llama/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from tqdm.auto import tqdm\n",
    "import blingfire as bf\n",
    "from __future__ import annotations\n",
    "from collections.abc import Iterable\n",
    "# import faiss\n",
    "# from faiss import write_index, read_index\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import ctypes\n",
    "libc = ctypes.CDLL(\"libc.so.6\")\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Union\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForMultipleChoice, TrainingArguments, Trainer\n",
    "from transformers.tokenization_utils_base import PreTrainedTokenizerBase, PaddingStrategy\n",
    "from torch.utils.data import DataLoader\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# df_train = pd.read_csv('/kaggle/input/kaggle-llm-science-exam/train.csv')\n",
    "# df_test  = pd.read_csv('/kaggle/input/kaggle-llm-science-exam/test.csv')\n",
    "# df_samp = pd.read_csv('/kaggle/input/kaggle-llm-science-exam/sample_submission.csv')\n",
    "# df_extra = pd.read_csv('/kaggle/input/additional-train-data-for-llm-science-exam/extra_train_set.csv')\n",
    "# df_extra_RO = pd.read_csv('/kaggle/input/additional-train-data-for-llm-science-exam/6000_train_examples.csv')\n",
    "df_train = pd.read_csv('data/train.csv')\n",
    "df_test  = pd.read_csv('data/test.csv')\n",
    "df_samp = pd.read_csv('data/sample_submission.csv')\n",
    "df_extra = pd.read_csv('data/extra_train_set.csv')\n",
    "df_extra_RO = pd.read_csv('data/6000_train_examples.csv')\n",
    "\n",
    "# We have two dataframes named df_train and df_extra with the same number of rows\n",
    "# Concatenate them\n",
    "concatenated_df = pd.concat([df_train])\n",
    "df_train.reset_index(inplace=True, drop=True)\n",
    "df_train.shape\n",
    "\n",
    "import random\n",
    "import pandas as pd\n",
    "from transformers import pipeline, AutoTokenizer,BertForMaskedLM\n",
    "\n",
    "# Load the BERT tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Initialize the pipeline for masked language modeling using BERT\n",
    "mlm_fill_mask = pipeline(task=\"fill-mask\", model=\"bert-base-uncased\")\n",
    "model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Function to perform data augmentation using different techniques\n",
    "def augment_data(original_df, num_augmented_rows):\n",
    "    augmented_data = []\n",
    "    original_rows = original_df.shape[0]\n",
    "\n",
    "    # Function for contextual word embeddings augmentation\n",
    "    def contextual_embeddings(text):\n",
    "        # Tokenize the text\n",
    "        tokenized_text = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "        # Find masked positions in the tokenized text\n",
    "        masked_positions = [i for i, token in enumerate(tokenized_text[\"input_ids\"][0]) if token == tokenizer.mask_token_id]\n",
    "\n",
    "        # If no masked positions found, return the original text\n",
    "        if not masked_positions:\n",
    "            return text\n",
    "\n",
    "        # Randomly select one of the masked positions\n",
    "        random_masked_position = random.choice(masked_positions)\n",
    "\n",
    "        # Predict the masked word using masked language modeling\n",
    "        masked_text = text.replace(\"[MASK]\", tokenizer.mask_token)\n",
    "        predicted_word = mlm_fill_mask(masked_text)[0][\"token_str\"]\n",
    "\n",
    "        # Replace the masked word in the text with the predicted word\n",
    "        augmented_text = text.replace(tokenizer.mask_token, predicted_word, 1)\n",
    "\n",
    "        return augmented_text\n",
    "\n",
    "    def get_synonyms(word, top_n=5):\n",
    "        text = f\"{word} is a [MASK] word.\"\n",
    "        inputs = tokenizer(text, return_tensors='pt')\n",
    "\n",
    "        # Get the index of the masked word\n",
    "        mask_idx = inputs['input_ids'][0].tolist().index(tokenizer.mask_token_id)\n",
    "\n",
    "        # Forward pass\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "        # Get the logits of the masked word\n",
    "        logits = outputs.logits[0, mask_idx]\n",
    "        \n",
    "        # Get the top_n words as synonyms\n",
    "        top_n_indices = logits.topk(top_n).indices\n",
    "        top_n_words = [tokenizer.convert_ids_to_tokens([idx])[0] for idx in top_n_indices]\n",
    "\n",
    "        return top_n_words\n",
    "\n",
    "    def augment_with_synonyms(text):\n",
    "        words = text.split()\n",
    "        augmented_words = []\n",
    "\n",
    "        for word in words:\n",
    "            # Use BERT to find synonyms if the word is not in our predefined dictionary\n",
    "            synonyms = get_synonyms(word.lower())\n",
    "            \n",
    "            # Randomly select a synonym\n",
    "            synonym = random.choice(synonyms)\n",
    "\n",
    "            # Replace the word with its synonym\n",
    "            augmented_words.append(synonym)\n",
    "\n",
    "        return \" \".join(augmented_words)\n",
    "\n",
    "    for _ in range(num_augmented_rows):\n",
    "        original_row = original_df.iloc[random.randint(0, original_rows - 1)]\n",
    "        augmented_row = original_row.copy()\n",
    "\n",
    "        # Apply augmentation techniques to \"prompt\"\n",
    "        augmented_row[\"prompt\"] = contextual_embeddings(original_row[\"prompt\"])\n",
    "\n",
    "        # Apply synonym replacement to answer choices (A, B, C, D, E)\n",
    "        for choice in [\"A\", \"B\", \"C\", \"D\", \"E\"]:\n",
    "            augmented_row[choice] = augment_with_synonyms(original_row[choice])\n",
    "\n",
    "        augmented_data.append(augmented_row)\n",
    "\n",
    "    return augmented_data\n",
    "\n",
    "# Assuming you have a concatenated_df dataframe with columns: \"prompt\", \"A\", \"B\", \"C\", \"D\", \"E\", and \"answer\"\n",
    "# Set the desired number of rows\n",
    "desired_rows = 500\n",
    "\n",
    "# Calculate the number of rows needed to achieve the desired total rows\n",
    "additional_rows = desired_rows - concatenated_df.shape[0]\n",
    "\n",
    "# Augment the data\n",
    "augmented_data = augment_data(concatenated_df, additional_rows)\n",
    "\n",
    "# Convert the augmented data to a dataframe\n",
    "augmented_df = pd.DataFrame(augmented_data)\n",
    "\n",
    "# Concatenate the original dataframe with the augmented dataframe to get the final dataframe with 3000 rows\n",
    "final_df = pd.concat([concatenated_df, augmented_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n"
     ]
    }
   ],
   "source": [
    "print(len(final_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sectionize_documents(documents: Iterable[str],\n",
    "                         document_ids: Iterable,\n",
    "                         disable_progress_bar: bool = False) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Obtains the sections of the imaging reports and returns only the \n",
    "    selected sections (defaults to FINDINGS, IMPRESSION, and ADDENDUM).\n",
    "\n",
    "    :param documents: Iterable containing documents which are strings\n",
    "    :param document_ids: Iterable containing document unique identifiers\n",
    "    :param disable_progress_bar: Flag to disable tqdm progress bar\n",
    "    :return: Pandas DataFrame containing the columns `document_id`, `text`, `offset`\n",
    "    \"\"\"\n",
    "    processed_documents = []\n",
    "    for document_id, document in tqdm(zip(document_ids, documents), total=len(documents), disable=disable_progress_bar):\n",
    "        row = {}\n",
    "        text, start, end = (document, 0, len(document))\n",
    "        row['document_id'] = document_id\n",
    "        row['text'] = text\n",
    "        row['offset'] = (start, end)\n",
    "\n",
    "        processed_documents.append(row)\n",
    "\n",
    "    _df = pd.DataFrame(processed_documents)\n",
    "    if _df.shape[0] > 0:\n",
    "        return _df.sort_values(['document_id', 'offset']).reset_index(drop=True)\n",
    "    else:\n",
    "        return _df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_documents(documents: Iterable[str],\n",
    "                      document_ids: Iterable,\n",
    "                      split_sentences: bool = True,\n",
    "                      filter_len: int = 9,\n",
    "                      disable_progress_bar: bool = False) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Main helper function to process documents from the EMR.\n",
    "\n",
    "    :param documents: Iterable containing documents which are strings\n",
    "    :param document_ids: Iterable containing document unique identifiers\n",
    "    :param document_type: String denoting the document type to be processed\n",
    "    :param document_sections: List of sections for a given document type to process\n",
    "    :param split_sentences: Flag to determine whether to further split sections into sentences\n",
    "    :param filter_len: Minimum character length of a sentence (otherwise filter out)\n",
    "    :param disable_progress_bar: Flag to disable tqdm progress bar\n",
    "    :return: Pandas DataFrame containing the columns `document_id`, `text`, `section`, `offset`\n",
    "    \"\"\"\n",
    "    \n",
    "    df = sectionize_documents(documents, document_ids, disable_progress_bar)\n",
    "\n",
    "    if split_sentences:\n",
    "        df = sentencize(df.text.values, \n",
    "                        df.document_id.values,\n",
    "                        df.offset.values, \n",
    "                        filter_len, \n",
    "                        disable_progress_bar)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIM_MODEL = '/kaggle/input/sentencetransformers-allminilml6v2/sentence-transformers_all-MiniLM-L6-v2'\n",
    "DEVICE = 0\n",
    "MAX_LENGTH = 384\n",
    "BATCH_SIZE = 32\n",
    "WIKI_PATH = \"/kaggle/input/wikipedia-20230701\"\n",
    "wiki_files = os.listdir(WIKI_PATH)\n",
    "\n",
    "test_df = pd.read_csv(\"/kaggle/input/kaggle-llm-science-exam/test.csv\").drop(\"id\", 1)\n",
    "\n",
    "## Combine all answers\n",
    "test_df['answer_all'] = test_df.apply(lambda x: \" \".join([x['A'], x['B'], x['C'], x['D'], x['E']]), axis=1)\n",
    "\n",
    "\n",
    "## Search using the prompt and answers to guide the search\n",
    "test_df['prompt_answer_stem'] = test_df['prompt'] + \" \" + test_df['answer_all']\n",
    "\n",
    "model = SentenceTransformer(SIM_MODEL, device='cuda')\n",
    "model.max_seq_length = MAX_LENGTH\n",
    "model = model.half()\n",
    "\n",
    "sentence_index = read_index(\"/kaggle/input/wikipedia-2023-07-faiss-index/wikipedia_202307.index\")\n",
    "prompt_embeddings = model.encode(test_df.prompt_answer_stem.values, batch_size=BATCH_SIZE, device=DEVICE, show_progress_bar=True, convert_to_tensor=True, normalize_embeddings=True)\n",
    "prompt_embeddings = prompt_embeddings.detach().cpu().numpy()\n",
    "_ = gc.collect()\n",
    "\n",
    "search_score, search_index = sentence_index.search(prompt_embeddings, 6)\n",
    "\n",
    "del sentence_index\n",
    "del prompt_embeddings\n",
    "_ = gc.collect()\n",
    "libc.malloc_trim(0)\n",
    "\n",
    "df = pd.read_parquet(\"/kaggle/input/wikipedia-20230701/wiki_2023_index.parquet\",\n",
    "                     columns=['id', 'file'])\n",
    "\n",
    "\n",
    "wikipedia_file_data = []\n",
    "\n",
    "for i, (scr, idx) in tqdm(enumerate(zip(search_score, search_index)), total=len(search_score)):\n",
    "    scr_idx = idx\n",
    "    _df = df.loc[scr_idx].copy()\n",
    "    _df['prompt_id'] = i\n",
    "    wikipedia_file_data.append(_df)\n",
    "wikipedia_file_data = pd.concat(wikipedia_file_data).reset_index(drop=True)\n",
    "wikipedia_file_data = wikipedia_file_data[['id', 'prompt_id', 'file']].drop_duplicates().sort_values(['file', 'id']).reset_index(drop=True)\n",
    "\n",
    "## Save memory - delete df since it is no longer necessary\n",
    "del df\n",
    "_ = gc.collect()\n",
    "libc.malloc_trim(0)\n",
    "\n",
    "wiki_text_data = []\n",
    "\n",
    "for file in tqdm(wikipedia_file_data.file.unique(), total=len(wikipedia_file_data.file.unique())):\n",
    "    _id = [str(i) for i in wikipedia_file_data[wikipedia_file_data['file']==file]['id'].tolist()]\n",
    "    _df = pd.read_parquet(f\"{WIKI_PATH}/{file}\", columns=['id', 'text'])\n",
    "\n",
    "    _df_temp = _df[_df['id'].isin(_id)].copy()\n",
    "    del _df\n",
    "    _ = gc.collect()\n",
    "    libc.malloc_trim(0)\n",
    "    wiki_text_data.append(_df_temp)\n",
    "wiki_text_data = pd.concat(wiki_text_data).drop_duplicates().reset_index(drop=True)\n",
    "_ = gc.collect()\n",
    "\n",
    "processed_wiki_text_data = process_documents(wiki_text_data.text.values, wiki_text_data.id.values)\n",
    "\n",
    "wiki_data_embeddings = model.encode(processed_wiki_text_data.text,\n",
    "                                    batch_size=BATCH_SIZE,\n",
    "                                    device=DEVICE,\n",
    "                                    show_progress_bar=True,\n",
    "                                    convert_to_tensor=True,\n",
    "                                    normalize_embeddings=True)#.half()\n",
    "wiki_data_embeddings = wiki_data_embeddings.detach().cpu().numpy()\n",
    "\n",
    "_ = gc.collect()\n",
    "\n",
    "question_embeddings = model.encode(test_df.prompt_answer_stem.values, batch_size=BATCH_SIZE, device=DEVICE, show_progress_bar=True, convert_to_tensor=True, normalize_embeddings=True)\n",
    "question_embeddings = question_embeddings.detach().cpu().numpy()\n",
    "\n",
    "## Parameter to determine how many relevant sentences to include\n",
    "NUM_SENTENCES_INCLUDE = 22\n",
    "\n",
    "## List containing just Context\n",
    "contexts = []\n",
    "\n",
    "for r in tqdm(test_df.itertuples(), total=len(test_df)):\n",
    "\n",
    "    prompt_id = r.Index\n",
    "\n",
    "    prompt_indices = processed_wiki_text_data[processed_wiki_text_data['document_id'].isin(wikipedia_file_data[wikipedia_file_data['prompt_id']==prompt_id]['id'].values)].index.values\n",
    "\n",
    "    if prompt_indices.shape[0] > 0:\n",
    "        prompt_index = faiss.index_factory(wiki_data_embeddings.shape[1], \"Flat\")\n",
    "        prompt_index.add(wiki_data_embeddings[prompt_indices])\n",
    "\n",
    "        context = \"\"\n",
    "        \n",
    "        ## Get the top matches\n",
    "        ss, ii = prompt_index.search(question_embeddings, NUM_SENTENCES_INCLUDE)\n",
    "        for _s, _i in zip(ss[prompt_id], ii[prompt_id]):\n",
    "            context += processed_wiki_text_data.loc[prompt_indices]['text'].iloc[_i] + \" \"\n",
    "        \n",
    "    contexts.append(context)\n",
    "    \n",
    "test_df['context'] = contexts\n",
    "test_df[[\"prompt\", \"context\", \"A\", \"B\", \"C\", \"D\", \"E\"]].to_csv(\"./text_context.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(\"test_context.csv\")\n",
    "test_df.index = list(range(len(test_df)))\n",
    "test_df['id'] = list(range(len(test_df)))\n",
    "test_df[\"prompt\"] = test_df[\"context\"].apply(lambda x: x[:2300]) + \" #### \" +  test_df[\"prompt\"]\n",
    "test_df['answer'] = 'A'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = \"/kaggle/input/llm-science-run-context-2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "model = AutoModelForMultipleChoice.from_pretrained(model_dir).cuda()\n",
    "model.eval()\n",
    "\n",
    "# We'll create a dictionary to convert option names (A, B, C, D, E) into indices and back again\n",
    "options = 'ABCDE'\n",
    "indices = list(range(5))\n",
    "\n",
    "option_to_index = {option: index for option, index in zip(options, indices)}\n",
    "index_to_option = {index: option for option, index in zip(options, indices)}\n",
    "\n",
    "def preprocess(example):\n",
    "    # The AutoModelForMultipleChoice class expects a set of question/answer pairs\n",
    "    # so we'll copy our question 5 times before tokenizing\n",
    "    first_sentence = [example['prompt']] * 5\n",
    "    second_sentence = []\n",
    "    for option in options:\n",
    "        second_sentence.append(example[option])\n",
    "    # Our tokenizer will turn our text into token IDs BERT can understand\n",
    "    tokenized_example = tokenizer(first_sentence, second_sentence, truncation=True)\n",
    "    tokenized_example['label'] = option_to_index[example['answer']]\n",
    "    return tokenized_example\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorForMultipleChoice:\n",
    "    tokenizer: PreTrainedTokenizerBase\n",
    "    padding: Union[bool, str, PaddingStrategy] = True\n",
    "    max_length: Optional[int] = None\n",
    "    pad_to_multiple_of: Optional[int] = None\n",
    "    \n",
    "    def __call__(self, features):\n",
    "        label_name = \"label\" if 'label' in features[0].keys() else 'labels'\n",
    "        labels = [feature.pop(label_name) for feature in features]\n",
    "        batch_size = len(features)\n",
    "        num_choices = len(features[0]['input_ids'])\n",
    "        flattened_features = [\n",
    "            [{k: v[i] for k, v in feature.items()} for i in range(num_choices)] for feature in features\n",
    "        ]\n",
    "        flattened_features = sum(flattened_features, [])\n",
    "        \n",
    "        batch = self.tokenizer.pad(\n",
    "            flattened_features,\n",
    "            padding=self.padding,\n",
    "            max_length=self.max_length,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        batch = {k: v.view(batch_size, num_choices, -1) for k, v in batch.items()}\n",
    "        batch['labels'] = torch.tensor(labels, dtype=torch.int64)\n",
    "        return batch\n",
    "    \n",
    "tokenized_test_dataset = Dataset.from_pandas(test_df[['id', 'prompt', 'A', 'B', 'C', 'D', 'E', 'answer']].drop(columns=['id'])).map(preprocess, remove_columns=['prompt', 'A', 'B', 'C', 'D', 'E', 'answer'])\n",
    "tokenized_test_dataset = tokenized_test_dataset.remove_columns([\"__index_level_0__\"])\n",
    "data_collator = DataCollatorForMultipleChoice(tokenizer=tokenizer)\n",
    "test_dataloader = DataLoader(tokenized_test_dataset, batch_size=1, shuffle=False, collate_fn=data_collator)\n",
    "\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
    "    return e_x / e_x.sum(axis=-1, keepdims = True)  \n",
    "\n",
    "%%time\n",
    "test_predictions = []\n",
    "for batch in test_dataloader:\n",
    "    for k in batch.keys():\n",
    "        batch[k] = batch[k].cuda()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**batch)\n",
    "    test_predictions.append(outputs.logits.cpu().detach())\n",
    "\n",
    "test_predictions = torch.cat(test_predictions)\n",
    "test_predictions = test_predictions.numpy()\n",
    "\n",
    "%%time\n",
    "\n",
    "model_dir = \"/kaggle/input/how-to-train-open-book-model-part-1/model_v2/\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "model = AutoModelForMultipleChoice.from_pretrained(model_dir).cuda()\n",
    "model.eval()\n",
    "\n",
    "test_predictions2 = []\n",
    "for batch in test_dataloader:\n",
    "    for k in batch.keys():\n",
    "        batch[k] = batch[k].cuda()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**batch)\n",
    "    test_predictions2.append(outputs.logits.cpu().detach())\n",
    "\n",
    "test_predictions2 = torch.cat(test_predictions2)\n",
    "test_predictions2 = test_predictions2.numpy()\n",
    "\n",
    "test_predictions = softmax (test_predictions) + softmax(test_predictions2)\n",
    "\n",
    "predictions_as_ids = np.argsort(-test_predictions, 1)\n",
    "\n",
    "predictions_as_answer_letters = np.array(list('ABCDE'))[predictions_as_ids]\n",
    "# predictions_as_answer_letters[:3]\n",
    "\n",
    "predictions_as_string = test_df['prediction'] = [\n",
    "    ' '.join(row) for row in predictions_as_answer_letters[:, :3]\n",
    "]\n",
    "\n",
    "submission = test_df[['id', 'prediction']]\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
